# VISION IRL
![image](https://github.com/user-attachments/assets/4a8f3359-7229-463a-a16e-fca07781dca6)


````markdown
# ğŸ‘ï¸â€ğŸ—¨ï¸ VISION IRL â€“ Offline Assistive AI for the Visually Impaired

**VISION IRL** is an **offline-first, privacy-focused assistive system** built to help visually impaired users interact with their surroundings through **AI-driven perception**. It runs entirely **locally** on SnapdragonÂ® X Elite hardware, making use of **Qualcommâ€™s NPU acceleration** through the **SnapdragonÂ® AI Stack** and **QNN (QualcommÂ® Neural Network)** tools.

---

## ğŸš€ Key Features

- ğŸ¯ Real-time Object Detection with **YOLOv8**
- ğŸ§ Pose Detection via **MediaPipe**
- ğŸ‘ï¸ Depth Perception using **Intel RealSenseâ„¢**
- ğŸ”Š Fully Offline Speech I/O using **Vosk + pyttsx3**
- ğŸ§  Face & Voice-based User Authentication
- âš™ï¸ Runs fully on-device â€” **no internet required**
- âš¡ Optimized for **Snapdragon X Elite** with NPU acceleration via **QNN + AI Hub**

---

## ğŸ’» System Requirements

| Component          | Requirement                         |
|--------------------|-------------------------------------|
| **Device**          | SnapdragonÂ® X Elite (e.g. Dell 7455)|
| **OS**              | Windows 11 ARM64                   |
| **Python**          | 3.12.x                             |
| **Camera**          | Intel RealSense D435i (or similar) |
| **NPU Runtime**     | SnapdragonÂ® AI SDK + QNN           |

---

## ğŸ“¦ Installation

### 1. Clone the Project
```bash
git clone https://github.com/Anuzka-Sharma/VISION-IRL-Copy.git
cd VISION-IRL-Copy
````

### 2. Create and Activate Virtual Environment

```bash
python -m venv venv
venv\Scripts\activate
```

### 3. Install Dependencies

```bash
pip install -r requirements.txt
```

If `PyAudio` fails:

```bash
pip install pipwin
pipwin install pyaudio
```

---

## ğŸ”Œ Using SnapdragonÂ® NPU (QNN) Acceleration

### âœ… Requirements

* [SnapdragonÂ® AI SDK](https://developer.qualcomm.com/software/ai-stack)
* [QualcommÂ® AI Hub account](https://aihub.qualcomm.com)
* Compatible `.onnx` or `.tflite` models (you can convert YOLO, Whisper, etc.)
* Optional: [AnythingLLM + NPU Edge Chat App](https://github.com/Qualcomm-AI-research)

---

### ğŸ› ï¸ Steps to Run with QNN Acceleration

1. **Install Snapdragon AI SDK for Windows**

   * Download from: [https://developer.qualcomm.com/software/ai-stack](https://developer.qualcomm.com/software/ai-stack)
   * Follow the SDK documentation to set up the QNN runtime.

2. **Convert Model for QNN**

   * Use AI Hub or QNN Tools to convert models to `.qnn` format:

   ```bash
   qnn-convert --onnx your_model.onnx --output-dir ./models/qnn
   ```

3. **Enable QNN Backend in Code**

   * In `object_detection.py` or `speech_model.py`:

   ```python
   from qti.ai.model.api import QNNEngine
   model = QNNEngine("models/qnn/yolov8.qnn", backend="npu")
   ```

4. **Verify Acceleration**

   * Use `qnn-inspect` or runtime logs to confirm that the NPU is running ops (e.g. `Compute Unit(s): npu (851 ops)`).

---

## ğŸ§ª Real-Time Capabilities

| Function         | Tech Used         | Accelerated                        |
| ---------------- | ----------------- | ---------------------------------- |
| Object Detection | YOLOv8 (ONNX/QNN) | âœ… (via QNN)                        |
| Speech-to-Text   | Whisper / Vosk    | âœ… (Whisper on QNN or CPU fallback) |
| Face Auth        | dlib / FaceNet    | âŒ (CPU-only)                       |
| Depth Estimation | RealSense SDK     | âŒ (native pipeline)                |
| Pose Detection   | MediaPipe         | âœ… (GPU/CPU)                        |

---

## ğŸ§  Offline AI Pipeline

```text
[Camera Input]
     â†“
[YOLOv8 on NPU] â†’ Object Labels
     â†“
[RealSense Depth Map] â†’ Distance
     â†“
[Pose Detection (MediaPipe)]
     â†“
[Speech Recognition (Vosk)] â† Microphone
     â†“
[Command Parser + TTS (pyttsx3)] â†’ Speaker
```

---

## ğŸ§‘â€ğŸ’» Project Structure

```
VISION-IRL/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ object_detection.py
â”‚   â”œâ”€â”€ speech_model.py
â”‚   â”œâ”€â”€ pose_module.py
â”‚   â””â”€â”€ auth/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ yolov8.onnx
â”‚   â””â”€â”€ yolov8.qnn
â”œâ”€â”€ assets/
â”œâ”€â”€ main.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ§° Toolchain Overview

| Tool                               | Purpose                    |
| ---------------------------------- | -------------------------- |
| `ultralytics`                      | YOLOv8 object detection    |
| `pyrealsense2`                     | RealSense D435i for depth  |
| `vosk`                             | Offline STT                |
| `pyttsx3`                          | Offline TTS                |
| `kivy`                             | GUI Framework              |
| `qnn` / `QNN SDK`                  | NPU inference acceleration |
| `SpeechRecognition`                | Optional fallback STT      |
| `torch`, `onnx`, `huggingface_hub` | Model development          |

---

## ğŸ§  LLM Chatbot Option (NPU)

You can also integrate a **local LLM chatbot** using:

* âœ… [Snapdragon NPU Edge Chat App](https://github.com/Qualcomm-AI-research)
* âœ… `AnythingLLM` with QNN-accelerated models
* Use this for **natural voice Q\&A**, **scene descriptions**, and **contextual help**

---

## ğŸ“œ License

MIT License â€“ free for non-commercial use with attribution.

---

## ğŸ¤ Acknowledgements

* QualcommÂ® AI Stack + AI Hub
* IntelÂ® RealSense SDK
* Ultralytics YOLOv8
* Whisper / Vosk
* MediaPipe by Google
* Kivy Open Source Framework

---

**Built for a world that everyone can experience. Offline. Real. Empowering. â€” VISION IRL**

```
